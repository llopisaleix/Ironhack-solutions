{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Lab\n",
    "\n",
    "You will find in this notebook some scrapy exercises to practise your scraping skills.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "- Check the response status code for each request to ensure you have obtained the intended content.\n",
    "- Print the response text in each request to understand the kind of info you are getting and its format.\n",
    "- Check for patterns in the response text to extract the data/info requested in each question.\n",
    "- Visit the urls below and take a look at their source code through Chrome DevTools. You'll need to identify the html tags, special class names, etc used in the html content you are expected to extract.\n",
    "\n",
    "**Resources**:\n",
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide)\n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [lxml lib](https://lxml.de/)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are the libraries and modules you may need. `requests`,  `BeautifulSoup` and `pandas` are already imported for you. If you prefer to use additional libraries feel free to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "#print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the names of the trending developers retrieved in the previous step.\n",
    "\n",
    "Your output should be a Python list of developer names. Each name should not contain any html tag.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Find out the html tag and class names used for the developer names. You can achieve this using Chrome DevTools.\n",
    "\n",
    "1. Use BeautifulSoup to extract all the html elements that contain the developer names.\n",
    "\n",
    "1. Use string manipulation techniques to replace whitespaces and linebreaks (i.e. `\\n`) in the *text* of each html element. Use a list to store the clean names.\n",
    "\n",
    "1. Print the list of names.\n",
    "\n",
    "Your output should look like below:\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (Á•ûÊ•ΩÂùÇË¶ö„ÄÖ)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thibault Duplessis (ornicar)',\n",
       " 'Jake Archibald (jakearchibald)',\n",
       " 'Stephen Roller (stephenroller)',\n",
       " 'David Rodr√≠guez (deivid-rodriguez)',\n",
       " 'Shawn Tabrizi (shawntabrizi)',\n",
       " 'Steven Allen (Stebalien)',\n",
       " 'Taylor Otwell (taylorotwell)',\n",
       " 'Sebasti√°n Ram√≠rez (tiangolo)',\n",
       " 'Leo Di Donato (leodido)',\n",
       " 'Felix Angelov (felangel)',\n",
       " 'Agniva De Sarker (agnivade)',\n",
       " 'Tyler Neely (spacejam)',\n",
       " 'Klaus Post (klauspost)',\n",
       " 'James Agnew (jamesagnew)',\n",
       " 'Dustin L. Howett (DHowett)',\n",
       " 'Anmol Sethi (nhooyr)',\n",
       " 'Yiyi Wang (shd101wyy)',\n",
       " 'Philipp Oppermann (phil-opp)',\n",
       " 'Ben Manes (ben-manes)',\n",
       " 'Stefan Prodan (stefanprodan)',\n",
       " 'Dries Vints (driesvints)',\n",
       " 'John Keiser (jkeiser)',\n",
       " 'Carlos Alexandro Becker (caarlos0)',\n",
       " 'Ines Montani (ines)',\n",
       " 'Brian Flad (bflad)']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "#Those are the tags I need. On h1 I have the name and the main repo and on 'p' I have the username. \n",
    "#See url = 'https://github.com/trending/developers' with the chrome dev tools and scroll over the name\n",
    "tags = ['h1', 'p']\n",
    "text = [element.text for element in soup.find_all(tags)][2:]\n",
    "#Check the results, the two first element are not needed (headers and presentations)\n",
    "#text\n",
    "\n",
    "new = []\n",
    "\n",
    "# #Iteration to clean strings\n",
    "for item in text: \n",
    "    x = item.strip()\n",
    "    new.append(x)\n",
    "\n",
    "# #Check the result\n",
    "new\n",
    "\n",
    "# # #Function to create chunks of name+username+repo\n",
    "def chunks(l, n):\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i+n]\n",
    "\n",
    "# # #I have to pass 3 to chunks to create lists of 3 items (name, user and repo)\n",
    "new_text = chunks(new, 3) \n",
    "#new_text\n",
    "# # #Convert results to a list\n",
    "newest_text = list(new_text)\n",
    "newest_text\n",
    "# # #Check the results\n",
    "\n",
    "\n",
    "result = []\n",
    "\n",
    "for i in range(len(newest_text)):\n",
    "        element = newest_text[i][0] + ' (' + newest_text[i][1] + ')'  \n",
    "        result.append(element)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = soup.find_all('h2',{'class':'f3 text-normal'});\n",
    "trending_devs = [dev.text.strip().replace(' ','').replace('\\n\\n', ' ') for dev in table];\n",
    "trending_devs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the trending Python repositories in GitHub.\n",
    "\n",
    "The steps to solve this problem is similar to the previous one except that you need to find out the repository names instead of developer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/python?since=daily'\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['minimaxir/big-list-of-naughty-strings', 'rusty1s/pytorch_geometric', 'espnet/espnet', 'public-apis/public-apis', 'donnemartin/system-design-primer', 'ranjian0/building_tool', 'sherlock-project/sherlocküîé', 'OpenMined/PySyft', 'open-mmlab/mmdetection', 'xillwillx/skiptracer', 'allenai/allennlp', 'explosion/spaCyüí´', 'zylo117/Yet-Another-EfficientDet-Pytorch', 'renatoviolin/next_word_prediction', 'anandpawara/Real_Time_Image_Animation', 'pytorch/fairseq', 'lyhue1991/eat_tensorflow2_in_30_days', 'zhanghang1989/ResNeSt', 'Rapptz/discord.py', 'google-research/big_transfer', 'TachibanaYoshino/AnimeGAN', 'shengqiangzhang/examples-of-web-crawlers', 'hunglc007/tensorflow-yolov4-tflite', 'ianzhao05/textshot', 'bitcoinbook/bitcoinbook']\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "articles = soup.find_all('article')\n",
    "repo = []\n",
    "articles\n",
    "for a in articles:\n",
    "    clean = a.text.strip().replace('\\n\\n','').split()\n",
    "    if clean[0] != 'Popular':\n",
    "        repo.append(clean[1] + clean[2] + clean[3])\n",
    "print(repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['minimaxir / big-list-of-naughty-strings',\n",
       " 'rusty1s / pytorch_geometric',\n",
       " 'espnet / espnet',\n",
       " 'public-apis / public-apis',\n",
       " 'donnemartin / system-design-primer',\n",
       " 'ranjian0 / building_tool',\n",
       " 'sherlock-project / sherlock',\n",
       " 'OpenMined / PySyft',\n",
       " 'open-mmlab / mmdetection',\n",
       " 'xillwillx / skiptracer',\n",
       " 'allenai / allennlp',\n",
       " 'explosion / spaCy',\n",
       " 'zylo117 / Yet-Another-EfficientDet-Pytorch',\n",
       " 'renatoviolin / next_word_prediction',\n",
       " 'anandpawara / Real_Time_Image_Animation',\n",
       " 'pytorch / fairseq',\n",
       " 'lyhue1991 / eat_tensorflow2_in_30_days',\n",
       " 'zhanghang1989 / ResNeSt',\n",
       " 'Rapptz / discord.py',\n",
       " 'google-research / big_transfer',\n",
       " 'TachibanaYoshino / AnimeGAN',\n",
       " 'shengqiangzhang / examples-of-web-crawlers',\n",
       " 'hunglc007 / tensorflow-yolov4-tflite',\n",
       " 'ianzhao05 / textshot',\n",
       " 'bitcoinbook / bitcoinbook']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "\n",
    "#On the h1 tag I find the name of the repo\n",
    "tags = ['h1']\n",
    "text = [element.text for element in soup.find_all(tags)][1:]\n",
    "#First element does not interest us. Check the result\n",
    "#text\n",
    "\n",
    "#Cleaning the results (step 1)\n",
    "new = []\n",
    "\n",
    "for item in text: \n",
    "    x = item.strip()     \n",
    "    new.append(x)\n",
    "\n",
    "#Check the results\n",
    "new\n",
    "\n",
    "#Cleaning (step 2)\n",
    "newest_list = []\n",
    "\n",
    "for element in new:\n",
    "    item = element.replace('\\n', '')\n",
    "    item2 = item.replace('     ', '')\n",
    "    newest_list.append(item2)\n",
    "\n",
    "newest_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display all the image links from Walt Disney wikipedia page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/Walt_Disney'\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "images = soup.find_all(\"img\")\n",
    "result = []\n",
    "for i in images:\n",
    "    result.append('https:' + i['src'])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve an arbitary Wikipedia page of \"Python\" and create a list of links on that page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://en.wikipedia.org/wiki/Python';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#mw-head\n",
      "#p-search\n",
      "https://en.wiktionary.org/wiki/Python\n",
      "https://en.wiktionary.org/wiki/python\n",
      "#Snakes\n",
      "#Ancient_Greece\n",
      "#Media_and_entertainment\n",
      "#Computing\n",
      "#Engineering\n",
      "#Roller_coasters\n",
      "#Vehicles\n",
      "#Weaponry\n",
      "#People\n",
      "#Other_uses\n",
      "#See_also\n",
      "/w/index.php?title=Python&action=edit&section=1\n",
      "/wiki/Pythonidae\n",
      "/wiki/Python_(genus)\n",
      "/w/index.php?title=Python&action=edit&section=2\n",
      "/wiki/Python_(mythology)\n",
      "/wiki/Python_of_Aenus\n",
      "/wiki/Python_(painter)\n",
      "/wiki/Python_of_Byzantium\n",
      "/wiki/Python_of_Catana\n",
      "/w/index.php?title=Python&action=edit&section=3\n",
      "/wiki/Python_(film)\n",
      "/wiki/Pythons_2\n",
      "/wiki/Monty_Python\n",
      "/wiki/Python_(Monty)_Pictures\n",
      "/w/index.php?title=Python&action=edit&section=4\n",
      "/wiki/Python_(programming_language)\n",
      "/wiki/CPython\n",
      "/wiki/CMU_Common_Lisp\n",
      "/wiki/PERQ#PERQ_3\n",
      "/w/index.php?title=Python&action=edit&section=5\n",
      "/w/index.php?title=Python&action=edit&section=6\n",
      "/wiki/Python_(Busch_Gardens_Tampa_Bay)\n",
      "/wiki/Python_(Coney_Island,_Cincinnati,_Ohio)\n",
      "/wiki/Python_(Efteling)\n",
      "/w/index.php?title=Python&action=edit&section=7\n",
      "/wiki/Python_(automobile_maker)\n",
      "/wiki/Python_(Ford_prototype)\n",
      "/w/index.php?title=Python&action=edit&section=8\n",
      "/wiki/Colt_Python\n",
      "/wiki/Python_(missile)\n",
      "/wiki/Python_(nuclear_primary)\n",
      "/w/index.php?title=Python&action=edit&section=9\n",
      "/wiki/Python_Anghelo\n",
      "/w/index.php?title=Python&action=edit&section=10\n",
      "/wiki/PYTHON\n",
      "/w/index.php?title=Python&action=edit&section=11\n",
      "/wiki/Cython\n",
      "/wiki/Pyton\n",
      "/wiki/File:Disambig_gray.svg\n",
      "/wiki/Help:Disambiguation\n",
      "https://en.wikipedia.org/w/index.php?title=Special:WhatLinksHere/Python&namespace=0\n",
      "https://en.wikipedia.org/w/index.php?title=Python&oldid=955934391\n",
      "/wiki/Help:Category\n",
      "/wiki/Category:Disambiguation_pages\n",
      "/wiki/Category:Disambiguation_pages_with_short_description\n",
      "/wiki/Category:All_article_disambiguation_pages\n",
      "/wiki/Category:All_disambiguation_pages\n",
      "/wiki/Category:Animal_common_name_disambiguation_pages\n",
      "/wiki/Special:MyTalk\n",
      "/wiki/Special:MyContributions\n",
      "/w/index.php?title=Special:CreateAccount&returnto=Python\n",
      "/w/index.php?title=Special:UserLogin&returnto=Python\n",
      "/wiki/Python\n",
      "/wiki/Talk:Python\n",
      "/wiki/Python\n",
      "/w/index.php?title=Python&action=edit\n",
      "/w/index.php?title=Python&action=history\n",
      "/wiki/Main_Page\n",
      "/wiki/Main_Page\n",
      "/wiki/Wikipedia:Contents\n",
      "/wiki/Wikipedia:Featured_content\n",
      "/wiki/Portal:Current_events\n",
      "/wiki/Special:Random\n",
      "https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&utm_medium=sidebar&utm_campaign=C13_en.wikipedia.org&uselang=en\n",
      "//shop.wikimedia.org\n",
      "/wiki/Help:Contents\n",
      "/wiki/Wikipedia:About\n",
      "/wiki/Wikipedia:Community_portal\n",
      "/wiki/Special:RecentChanges\n",
      "//en.wikipedia.org/wiki/Wikipedia:Contact_us\n",
      "/wiki/Special:WhatLinksHere/Python\n",
      "/wiki/Special:RecentChangesLinked/Python\n",
      "/wiki/Wikipedia:File_Upload_Wizard\n",
      "/wiki/Special:SpecialPages\n",
      "/w/index.php?title=Python&oldid=955934391\n",
      "/w/index.php?title=Python&action=info\n",
      "https://www.wikidata.org/wiki/Special:EntityPage/Q747452\n",
      "/w/index.php?title=Special:CiteThisPage&page=Python&id=955934391&wpFormIdentifier=titleform\n",
      "https://commons.wikimedia.org/wiki/Category:Python\n",
      "/w/index.php?title=Special:ElectronPdf&page=Python&action=show-download-screen\n",
      "/w/index.php?title=Python&printable=yes\n",
      "https://af.wikipedia.org/wiki/Python\n",
      "https://als.wikipedia.org/wiki/Python\n",
      "https://ar.wikipedia.org/wiki/%D8%A8%D8%A7%D9%8A%D8%AB%D9%88%D9%86\n",
      "https://az.wikipedia.org/wiki/Python\n",
      "https://bn.wikipedia.org/wiki/%E0%A6%AA%E0%A6%BE%E0%A6%87%E0%A6%A5%E0%A6%A8_(%E0%A6%A6%E0%A7%8D%E0%A6%AC%E0%A7%8D%E0%A6%AF%E0%A6%B0%E0%A7%8D%E0%A6%A5%E0%A6%A4%E0%A6%BE_%E0%A6%A8%E0%A6%BF%E0%A6%B0%E0%A6%B8%E0%A6%A8)\n",
      "https://be.wikipedia.org/wiki/Python\n",
      "https://bg.wikipedia.org/wiki/%D0%9F%D0%B8%D1%82%D0%BE%D0%BD_(%D0%BF%D0%BE%D1%8F%D1%81%D0%BD%D0%B5%D0%BD%D0%B8%D0%B5)\n",
      "https://cs.wikipedia.org/wiki/Python_(rozcestn%C3%ADk)\n",
      "https://da.wikipedia.org/wiki/Python\n",
      "https://de.wikipedia.org/wiki/Python\n",
      "https://eo.wikipedia.org/wiki/Pitono_(apartigilo)\n",
      "https://eu.wikipedia.org/wiki/Python_(argipena)\n",
      "https://fa.wikipedia.org/wiki/%D9%BE%D8%A7%DB%8C%D8%AA%D9%88%D9%86\n",
      "https://fr.wikipedia.org/wiki/Python\n",
      "https://ko.wikipedia.org/wiki/%ED%8C%8C%EC%9D%B4%EC%84%A0\n",
      "https://hr.wikipedia.org/wiki/Python_(razdvojba)\n",
      "https://io.wikipedia.org/wiki/Pitono\n",
      "https://id.wikipedia.org/wiki/Python\n",
      "https://ia.wikipedia.org/wiki/Python_(disambiguation)\n",
      "https://is.wikipedia.org/wiki/Python_(a%C3%B0greining)\n",
      "https://it.wikipedia.org/wiki/Python_(disambigua)\n",
      "https://he.wikipedia.org/wiki/%D7%A4%D7%99%D7%AA%D7%95%D7%9F\n",
      "https://ka.wikipedia.org/wiki/%E1%83%9E%E1%83%98%E1%83%97%E1%83%9D%E1%83%9C%E1%83%98_(%E1%83%9B%E1%83%A0%E1%83%90%E1%83%95%E1%83%90%E1%83%9A%E1%83%9B%E1%83%9C%E1%83%98%E1%83%A8%E1%83%95%E1%83%9C%E1%83%94%E1%83%9A%E1%83%9D%E1%83%95%E1%83%90%E1%83%9C%E1%83%98)\n",
      "https://kg.wikipedia.org/wiki/Mboma_(nyoka)\n",
      "https://la.wikipedia.org/wiki/Python_(discretiva)\n",
      "https://lb.wikipedia.org/wiki/Python\n",
      "https://hu.wikipedia.org/wiki/Python_(egy%C3%A9rtelm%C5%B1s%C3%ADt%C5%91_lap)\n",
      "https://mr.wikipedia.org/wiki/%E0%A4%AA%E0%A4%BE%E0%A4%AF%E0%A4%A5%E0%A5%89%E0%A4%A8_(%E0%A4%86%E0%A4%9C%E0%A5%8D%E0%A4%9E%E0%A4%BE%E0%A4%B5%E0%A4%B2%E0%A5%80_%E0%A4%AD%E0%A4%BE%E0%A4%B7%E0%A4%BE)\n",
      "https://nl.wikipedia.org/wiki/Python\n",
      "https://ja.wikipedia.org/wiki/%E3%83%91%E3%82%A4%E3%82%BD%E3%83%B3\n",
      "https://no.wikipedia.org/wiki/Pyton\n",
      "https://pl.wikipedia.org/wiki/Pyton\n",
      "https://pt.wikipedia.org/wiki/Python_(desambigua%C3%A7%C3%A3o)\n",
      "https://ru.wikipedia.org/wiki/Python_(%D0%B7%D0%BD%D0%B0%D1%87%D0%B5%D0%BD%D0%B8%D1%8F)\n",
      "https://sk.wikipedia.org/wiki/Python\n",
      "https://sr.wikipedia.org/wiki/%D0%9F%D0%B8%D1%82%D0%BE%D0%BD_(%D0%B2%D0%B8%D1%88%D0%B5%D0%B7%D0%BD%D0%B0%D1%87%D0%BD%D0%B0_%D0%BE%D0%B4%D1%80%D0%B5%D0%B4%D0%BD%D0%B8%D1%86%D0%B0)\n",
      "https://sh.wikipedia.org/wiki/Python\n",
      "https://fi.wikipedia.org/wiki/Python\n",
      "https://sv.wikipedia.org/wiki/Pyton\n",
      "https://th.wikipedia.org/wiki/%E0%B9%84%E0%B8%9E%E0%B8%97%E0%B8%AD%E0%B8%99\n",
      "https://tr.wikipedia.org/wiki/Python\n",
      "https://uk.wikipedia.org/wiki/%D0%9F%D1%96%D1%84%D0%BE%D0%BD\n",
      "https://ur.wikipedia.org/wiki/%D9%BE%D8%A7%D8%A6%DB%8C%D8%AA%DA%BE%D9%88%D9%86\n",
      "https://vi.wikipedia.org/wiki/Python\n",
      "https://zh.wikipedia.org/wiki/Python_(%E6%B6%88%E6%AD%A7%E4%B9%89)\n",
      "https://www.wikidata.org/wiki/Special:EntityPage/Q747452#sitelinks-wikipedia\n",
      "//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License\n",
      "//creativecommons.org/licenses/by-sa/3.0/\n",
      "//foundation.wikimedia.org/wiki/Terms_of_Use\n",
      "//foundation.wikimedia.org/wiki/Privacy_policy\n",
      "//www.wikimediafoundation.org/\n",
      "https://foundation.wikimedia.org/wiki/Privacy_policy\n",
      "/wiki/Wikipedia:About\n",
      "/wiki/Wikipedia:General_disclaimer\n",
      "//en.wikipedia.org/wiki/Wikipedia:Contact_us\n",
      "https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute\n",
      "https://stats.wikimedia.org/#/en.wikipedia.org\n",
      "https://foundation.wikimedia.org/wiki/Cookie_statement\n",
      "//en.m.wikipedia.org/w/index.php?title=Python&mobileaction=toggle_view_mobile\n",
      "https://wikimediafoundation.org/\n",
      "https://www.mediawiki.org/\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "table = soup.find_all('a')\n",
    "for link in table:\n",
    "    if 'href' in link.attrs:\n",
    "        print(link['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the number of titles that have changed in the United States Code since its last release point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'http://uscode.house.gov/download/download.shtml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "#When parsing, I see the only class different when a title has changes is usctitlechanged. Therefore I have to count those\n",
    "txt = requests.get(url).text;\n",
    "count = txt.count('class=\"usctitlechanged\"');\n",
    "print(f'Number of titles changed: {count}');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find a Python list with the top ten FBI's Most Wanted names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.fbi.gov/wanted/topten'\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "names = soup.find_all(\"h3\", attrs={\"class\":\"title\"})\n",
    "result = [n.text.strip() for n in names]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Display the 20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.emsc-csem.org/Earthquake/'\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "html = requests.get(url).content;\n",
    "soup = BeautifulSoup(html, \"lxml\");\n",
    "earthquakes = soup.find('tbody', {'id': 'tbody'}).find_all(\"tr\");\n",
    "\n",
    "nelem = 20;\n",
    "latest_earthquakes = [];\n",
    "    \n",
    "for earthquake in earthquakes[:nelem]:\n",
    "    # Date and time\n",
    "    date, time = earthquake.find('td', {'class': 'tabev6'}).find('a').text.split();\n",
    "    # Latitude and longitude\n",
    "    lat_deg, lon_deg = earthquake.find_all('td', {'class': 'tabev1'});\n",
    "    lat_dir, lon_dir, magnitude = earthquake.find_all('td', {'class': 'tabev2'});\n",
    "    lat_deg = f\"{lat_deg.text.strip()} {lat_dir.text.strip()}\";\n",
    "    lon_deg = f\"{lon_deg.text.strip()} {lon_dir.text.strip()}\";\n",
    "    # Region\n",
    "    region = earthquake.find('td', {'class': 'tb_region'}).text.strip();\n",
    "    # Create list of information and append\n",
    "    earthquake_summary = [date, time, lat_deg , lon_deg, region];\n",
    "    latest_earthquakes.append(earthquake_summary);\n",
    "    \n",
    "df = pd.DataFrame(latest_earthquakes, columns=['Date', 'Time', 'Latitude', 'Longitude', 'Region']);\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count the number of tweets by a given Twitter account.\n",
    "Ask the user for the handle (@handle) of a twitter account. You will need to include a ***try/except block*** for account names not found. \n",
    "<br>***Hint:*** the program should count the number of tweets for any provided account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please, input your username: ironhackams\n",
      "ironhackams has 114 number of tweets.\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "username = input('Please, input your username: ')\n",
    "html = requests.get(url + username).content\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "try:\n",
    "    tweet_box = soup.find('li', {'class':'ProfileNav-item ProfileNav-item--tweets is-active'});\n",
    "    tweets = tweet_box.find('a').find('span', {'class':'ProfileNav-value'});\n",
    "    print(\"{} has {} number of tweets.\".format(username, tweets.get('data-count')))\n",
    "except:\n",
    "    print('Account name not found...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of followers of a given twitter account\n",
    "Ask the user for the handle (@handle) of a twitter account. You will need to include a ***try/except block*** for account names not found. \n",
    "<br>***Hint:*** the program should count the followers for any provided account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please, input your username: ironhackams\n",
      "ironhackams has 203 followers.\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "username = input('Please, input your username: ')\n",
    "html = requests.get(url + username).content;\n",
    "soup = BeautifulSoup(html, \"lxml\");\n",
    "\n",
    "try:\n",
    "    tweet_box = soup.find('li', {'class':'ProfileNav-item ProfileNav-item--followers'});\n",
    "    tweets = tweet_box.find('a').find('span', {'class':'ProfileNav-value'});\n",
    "    print(\"{} has {} followers.\".format(username, tweets.get('data-count')))\n",
    "except:\n",
    "    print('Account name not found...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List all language names and number of related articles in the order they appear in wikipedia.org."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.wikipedia.org/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English\n",
      "6¬†085¬†000+ articles\n",
      "Espa√±ol\n",
      "1¬†601¬†000+ art√≠culos\n",
      "Êó•Êú¨Ë™û\n",
      "1¬†208¬†000+ Ë®ò‰∫ã\n",
      "Deutsch\n",
      "2¬†436¬†000+ Artikel\n",
      "–†—É—Å—Å–∫–∏–π\n",
      "1¬†629¬†000+ —Å—Ç–∞—Ç–µ–π\n",
      "Fran√ßais\n",
      "2¬†219¬†000+ articles\n",
      "Italiano\n",
      "1¬†609¬†000+ voci\n",
      "‰∏≠Êñá\n",
      "1¬†121¬†000+ Ê¢ùÁõÆ\n",
      "Portugu√™s\n",
      "1¬†033¬†000+ artigos\n",
      "Polski\n",
      "1¬†411¬†000+ hase≈Ç\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "languages = soup.find_all('a', {'class': 'link-box'})\n",
    "for language in languages:\n",
    "    print(language.text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A list with the different kind of datasets available in data.gov.uk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://data.gov.uk/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business and economy\n",
      "Crime and justice\n",
      "Defence\n",
      "Education\n",
      "Environment\n",
      "Government\n",
      "Government spending\n",
      "Health\n",
      "Mapping\n",
      "Society\n",
      "Towns and cities\n",
      "Transport\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html,\"lxml\")\n",
    "topics = soup.findAll('h2')\n",
    "for topic in topics:\n",
    "    print(topic.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the top 10 languages by number of native speakers stored in a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mandarin Chinese\n",
      "Sino-Tibetan\n",
      "Sinitic\n",
      "Spanish\n",
      "Indo-European\n",
      "Romance\n",
      "English\n",
      "Indo-European\n",
      "Germanic\n",
      "Hindi\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html,\"lxml\")\n",
    "languages = soup.find('table', {'class': 'wikitable sortable'}).find_all('a', attrs = {'title' : True});\n",
    "\n",
    "for i in range(10):\n",
    "    print(languages[i].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "#### Scrape a certain number of tweets of a given Twitter account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please, input your username: ironhackams\n",
      "Input number of tweets to scrape: 5\n",
      "\n",
      "Ironhack Amsterdam @ironhackAMS 14 mei\n",
      "Since the beginning of #covid19 , at Ironhack we have switched the format of the bootcamps to remote to keep the #ironhackers safe and sound Wondering what the remote bootcamps are all about? Here are 5 differences between #remote and #online courses\n",
      "https://soo.nr/mGaj¬†pic.twitter.com/oC33uQjN40\n",
      "0 antwoorden     0 retweets     0 vind-ik-leuks\n",
      "\n",
      "Ironhack Amsterdam @ironhackAMS 4 dec. 2019\n",
      "We have amazing news for you! Over $800,000 in scholarships to attend ANY one of our 9 global campuses  Apply Today! http://x.ea.com/61724¬†https://twitter.com/TheSims/status/1201940090517426177¬†‚Ä¶\n",
      "0 antwoorden     0 retweets     2 vind-ik-leuks\n",
      "\n",
      "Ironhack Amsterdam @ironhackAMS 2 apr. 2019\n",
      "Why I went off the beaten track and followed a web development bootcamp? - An alumni story by Matt Hamers\n",
      "\n",
      "#webdevelopment #ironhack #Fullstack #coding #bootcamp #Alumni \n",
      "\n",
      "Read the article in the link below!\n",
      "https://buff.ly/2I01rvT¬†pic.twitter.com/nsvS50BTU6\n",
      "0 antwoorden     0 retweets     0 vind-ik-leuks\n",
      "\n",
      "Ironhack Amsterdam @ironhackAMS 25 mrt. 2019\n",
      "What is the best design tool to use for UX/UI? Read it in this article by Marcos Cisneros ?\n",
      "\n",
      "#UXUI #design #designtools #bootcamp #ironhack \n",
      "\n",
      "click the link below:https://buff.ly/2uoIiM8¬†\n",
      "0 antwoorden     0 retweets     2 vind-ik-leuks\n",
      "\n",
      "Ironhack Amsterdam @ironhackAMS 15 mrt. 2019\n",
      "What you should know for your first web development job? \n",
      "Read it in this article by Gijs LeBesque! \n",
      "#webdevelopement #developer #code #interview #ironhack #bootcamp \n",
      "\n",
      "click the link below:\n",
      "https://buff.ly/2XTuR4q¬†pic.twitter.com/iRgXXarnsR\n",
      "0 antwoorden     0 retweets     1 vind-ik-leuk\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "username = input('Please, input your username: ')\n",
    "n_tweets = int(input('Input number of tweets to scrape: '))\n",
    "html = requests.get(url + username).content;\n",
    "soup = BeautifulSoup(html, \"lxml\");\n",
    "\n",
    "all_tweets = soup.find_all('div', {'class':'tweet'})\n",
    "\n",
    "if all_tweets:\n",
    "    for tweet in all_tweets[0:n_tweets]:\n",
    "        name = tweet.find('span', {'class': 'FullNameGroup'}).find('strong')\n",
    "        username = tweet.find('span', {'class': 'username'})\n",
    "        time = tweet.find('small', {'class': 'time'})\n",
    "        content = tweet.find('p', {'class': 'TweetTextSize TweetTextSize--normal js-tweet-text tweet-text'})\n",
    "        statistics = tweet.find('div', {'class': 'ProfileTweet-actionCountList u-hiddenVisually'})\n",
    "        \n",
    "        print(f'\\n{name.text} {username.text} {time.text.strip()}')\n",
    "        print(content.text)\n",
    "        print(statistics.text.strip().replace('\\n', ' '))\n",
    "else:\n",
    "    print('Account name not found or tweet list is empty...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display IMDB's top 250 data (movie name, initial release, director name and stars) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url = 'https://www.imdb.com/chart/top'\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Release</th>\n",
       "      <th>Director</th>\n",
       "      <th>Actors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "      <td>1994</td>\n",
       "      <td>Frank Darabont</td>\n",
       "      <td>Tim Robbins &amp;  Morgan Freeman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Godfather</td>\n",
       "      <td>1972</td>\n",
       "      <td>Francis Ford Coppola</td>\n",
       "      <td>Marlon Brando &amp;  Al Pacino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Godfather: Part II</td>\n",
       "      <td>1974</td>\n",
       "      <td>Francis Ford Coppola</td>\n",
       "      <td>Al Pacino &amp;  Robert De Niro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Dark Knight</td>\n",
       "      <td>2008</td>\n",
       "      <td>Christopher Nolan</td>\n",
       "      <td>Christian Bale &amp;  Heath Ledger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12 Angry Men</td>\n",
       "      <td>1957</td>\n",
       "      <td>Sidney Lumet</td>\n",
       "      <td>Henry Fonda &amp;  Lee J. Cobb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>Mandariinid</td>\n",
       "      <td>2013</td>\n",
       "      <td>Zaza Urushadze</td>\n",
       "      <td>Lembit Ulfsak &amp;  Elmo N√ºganen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>Aladdin</td>\n",
       "      <td>1992</td>\n",
       "      <td>Ron Clements</td>\n",
       "      <td>Scott Weinger &amp;  Robin Williams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>Lagaan: Once Upon a Time in India</td>\n",
       "      <td>2001</td>\n",
       "      <td>Ashutosh Gowariker</td>\n",
       "      <td>Aamir Khan &amp;  Raghuvir Yadav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>PK</td>\n",
       "      <td>2014</td>\n",
       "      <td>Rajkumar Hirani</td>\n",
       "      <td>Aamir Khan &amp;  Anushka Sharma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>Trois couleurs: Rouge</td>\n",
       "      <td>1994</td>\n",
       "      <td>Krzysztof Kieslowski</td>\n",
       "      <td>Ir√®ne Jacob &amp;  Jean-Louis Trintignant</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Title Release              Director  \\\n",
       "0             The Shawshank Redemption    1994        Frank Darabont   \n",
       "1                        The Godfather    1972  Francis Ford Coppola   \n",
       "2               The Godfather: Part II    1974  Francis Ford Coppola   \n",
       "3                      The Dark Knight    2008     Christopher Nolan   \n",
       "4                         12 Angry Men    1957          Sidney Lumet   \n",
       "..                                 ...     ...                   ...   \n",
       "245                        Mandariinid    2013        Zaza Urushadze   \n",
       "246                            Aladdin    1992          Ron Clements   \n",
       "247  Lagaan: Once Upon a Time in India    2001    Ashutosh Gowariker   \n",
       "248                                 PK    2014       Rajkumar Hirani   \n",
       "249              Trois couleurs: Rouge    1994  Krzysztof Kieslowski   \n",
       "\n",
       "                                     Actors  \n",
       "0             Tim Robbins &  Morgan Freeman  \n",
       "1                Marlon Brando &  Al Pacino  \n",
       "2               Al Pacino &  Robert De Niro  \n",
       "3            Christian Bale &  Heath Ledger  \n",
       "4                Henry Fonda &  Lee J. Cobb  \n",
       "..                                      ...  \n",
       "245           Lembit Ulfsak &  Elmo N√ºganen  \n",
       "246         Scott Weinger &  Robin Williams  \n",
       "247            Aamir Khan &  Raghuvir Yadav  \n",
       "248            Aamir Khan &  Anushka Sharma  \n",
       "249   Ir√®ne Jacob &  Jean-Louis Trintignant  \n",
       "\n",
       "[250 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "html = requests.get(url).content;\n",
    "soup = BeautifulSoup(html, \"lxml\");\n",
    "\n",
    "movies = soup.find_all('td', {'class':'titleColumn'})\n",
    "titles = [movie.find('a').text for movie in movies]\n",
    "years = [movie.find('span').text[1:-1] for movie in movies]\n",
    "directors = [movie.find('a').get('title').split(',')[0][:-7] for movie in movies]\n",
    "actors = [' & '.join(movie.find('a').get('title').split(',')[1:]) for movie in movies]\n",
    "\n",
    "movies_dict = {'Title': titles, 'Release': years, 'Director': directors, 'Actors': actors}\n",
    "\n",
    "movies_df = pd.DataFrame(movies_dict)\n",
    "movies_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the movie name, year and a brief summary of the top 10 random movies (IMDB) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the url you will scrape in this exercise\n",
    "url = 'http://www.imdb.com/chart/top'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Release</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Portrait de la jeune fille en feu</td>\n",
       "      <td>2019</td>\n",
       "      <td>On an isolated island in Brittany at the end o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Barry Lyndon</td>\n",
       "      <td>1975</td>\n",
       "      <td>An Irish rogue wins the heart of a rich widow ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Butch Cassidy and the Sundance Kid</td>\n",
       "      <td>1969</td>\n",
       "      <td>Wyoming, early 1900s. Butch Cassidy and The Su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Andhadhun</td>\n",
       "      <td>2018</td>\n",
       "      <td>A series of mysterious events change the life ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rocky</td>\n",
       "      <td>1976</td>\n",
       "      <td>A small-time boxer gets a supremely rare chanc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Lock, Stock and Two Smoking Barrels</td>\n",
       "      <td>1998</td>\n",
       "      <td>A botched card game in London triggers four fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Idi i smotri</td>\n",
       "      <td>1985</td>\n",
       "      <td>After finding an old rifle, a young boy joins ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Dial M for Murder</td>\n",
       "      <td>1954</td>\n",
       "      <td>A tennis player tries to arrange his wife's mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Full Metal Jacket</td>\n",
       "      <td>1987</td>\n",
       "      <td>A pragmatic U.S. Marine observes the dehumaniz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Faa yeung nin wa</td>\n",
       "      <td>2000</td>\n",
       "      <td>Two neighbors, a woman and a man, form a stron...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Title Release  \\\n",
       "0    Portrait de la jeune fille en feu    2019   \n",
       "1                         Barry Lyndon    1975   \n",
       "2   Butch Cassidy and the Sundance Kid    1969   \n",
       "3                            Andhadhun    2018   \n",
       "4                                Rocky    1976   \n",
       "5  Lock, Stock and Two Smoking Barrels    1998   \n",
       "6                         Idi i smotri    1985   \n",
       "7                    Dial M for Murder    1954   \n",
       "8                    Full Metal Jacket    1987   \n",
       "9                     Faa yeung nin wa    2000   \n",
       "\n",
       "                                             Summary  \n",
       "0  On an isolated island in Brittany at the end o...  \n",
       "1  An Irish rogue wins the heart of a rich widow ...  \n",
       "2  Wyoming, early 1900s. Butch Cassidy and The Su...  \n",
       "3  A series of mysterious events change the life ...  \n",
       "4  A small-time boxer gets a supremely rare chanc...  \n",
       "5  A botched card game in London triggers four fr...  \n",
       "6  After finding an old rifle, a young boy joins ...  \n",
       "7  A tennis player tries to arrange his wife's mu...  \n",
       "8  A pragmatic U.S. Marine observes the dehumaniz...  \n",
       "9  Two neighbors, a woman and a man, form a stron...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "\n",
    "from random import shuffle;\n",
    "\n",
    "n_random = 10;\n",
    "\n",
    "html = requests.get(url).content;\n",
    "soup = BeautifulSoup(html, \"lxml\");\n",
    "movies = soup.find_all('td', {'class':'titleColumn'})\n",
    "\n",
    "shuffle(movies)\n",
    "\n",
    "titles = [movie.find('a').text for movie in movies[0:n_random]]\n",
    "years = [movie.find('span').text[1:-1] for movie in movies[0:n_random]]\n",
    "links_to_movies = [movie.find('a').get('href') for movie in movies[0:n_random]]\n",
    "\n",
    "summary = []\n",
    "for link in links_to_movies:\n",
    "    html = requests.get('https://www.imdb.com' + link).content;\n",
    "    soup = BeautifulSoup(html, \"lxml\");\n",
    "    summary.append(soup.find('div', {'class':'summary_text'}).text.strip());\n",
    "\n",
    "movies_dict = {'Title': titles, 'Release': years, 'Summary': summary}\n",
    "\n",
    "movies_df = pd.DataFrame(movies_dict)\n",
    "movies_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the live weather report (temperature, wind speed, description and weather) of a given city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the city: Barcelona\n",
      "\n",
      "Barcelona's temperature: 25.27¬∞C \n",
      "Wind speed: 3.6 m/s\n",
      "Description: Clear sky\n",
      "Weather: Clear\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "\n",
    "city = input('Enter the city: ').lower();\n",
    "url = 'http://api.openweathermap.org/data/2.5/weather?'+'q='+city+'&APPID=b35975e18dc93725acb092f7272cc6b8&units=metric'\n",
    "weather_json = requests.get(url).json()\n",
    "\n",
    "print(\"\\n{}'s temperature: {}¬∞C \".format(city.capitalize(), weather_json['main']['temp']))\n",
    "print(\"Wind speed: {} m/s\".format(weather_json['wind']['speed']))\n",
    "print(\"Description: {}\".format(weather_json['weather'][0]['description'].capitalize()))\n",
    "print(\"Weather: {}\".format(weather_json['weather'][0]['main'].capitalize()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the book name, price and stock availability as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise. \n",
    "# It is a fictional bookstore created to be scraped. \n",
    "url = 'http://books.toscrape.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Price</th>\n",
       "      <th>Stock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Light in the ...</td>\n",
       "      <td>¬£51.77</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tipping the Velvet</td>\n",
       "      <td>¬£53.74</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Soumission</td>\n",
       "      <td>¬£50.10</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sharp Objects</td>\n",
       "      <td>¬£47.82</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sapiens: A Brief History ...</td>\n",
       "      <td>¬£54.23</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Requiem Red</td>\n",
       "      <td>¬£22.65</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Dirty Little Secrets ...</td>\n",
       "      <td>¬£33.34</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Coming Woman: A ...</td>\n",
       "      <td>¬£17.93</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The Boys in the ...</td>\n",
       "      <td>¬£22.60</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Black Maria</td>\n",
       "      <td>¬£52.15</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Starving Hearts (Triangular Trade ...</td>\n",
       "      <td>¬£13.99</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Shakespeare's Sonnets</td>\n",
       "      <td>¬£20.66</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Set Me Free</td>\n",
       "      <td>¬£17.46</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Scott Pilgrim's Precious Little ...</td>\n",
       "      <td>¬£52.29</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Rip it Up and ...</td>\n",
       "      <td>¬£35.02</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Our Band Could Be ...</td>\n",
       "      <td>¬£57.25</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Olio</td>\n",
       "      <td>¬£23.88</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Mesaerion: The Best Science ...</td>\n",
       "      <td>¬£37.59</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Libertarianism for Beginners</td>\n",
       "      <td>¬£51.33</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>It's Only the Himalayas</td>\n",
       "      <td>¬£45.17</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Title   Price     Stock\n",
       "0                      A Light in the ...  ¬£51.77  In stock\n",
       "1                      Tipping the Velvet  ¬£53.74  In stock\n",
       "2                              Soumission  ¬£50.10  In stock\n",
       "3                           Sharp Objects  ¬£47.82  In stock\n",
       "4            Sapiens: A Brief History ...  ¬£54.23  In stock\n",
       "5                         The Requiem Red  ¬£22.65  In stock\n",
       "6            The Dirty Little Secrets ...  ¬£33.34  In stock\n",
       "7                 The Coming Woman: A ...  ¬£17.93  In stock\n",
       "8                     The Boys in the ...  ¬£22.60  In stock\n",
       "9                         The Black Maria  ¬£52.15  In stock\n",
       "10  Starving Hearts (Triangular Trade ...  ¬£13.99  In stock\n",
       "11                  Shakespeare's Sonnets  ¬£20.66  In stock\n",
       "12                            Set Me Free  ¬£17.46  In stock\n",
       "13    Scott Pilgrim's Precious Little ...  ¬£52.29  In stock\n",
       "14                      Rip it Up and ...  ¬£35.02  In stock\n",
       "15                  Our Band Could Be ...  ¬£57.25  In stock\n",
       "16                                   Olio  ¬£23.88  In stock\n",
       "17        Mesaerion: The Best Science ...  ¬£37.59  In stock\n",
       "18           Libertarianism for Beginners  ¬£51.33  In stock\n",
       "19                It's Only the Himalayas  ¬£45.17  In stock"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "html = requests.get(url).content;\n",
    "soup = BeautifulSoup(html, \"lxml\");\n",
    "books = soup.find_all('article', {'class': 'product_pod'})\n",
    "\n",
    "titles = [book.find('h3').text for book in books];\n",
    "prices = [book.find('p', {'class': 'price_color'}).text for book in books];\n",
    "stock = [book.find('p', {'class': 'instock availability'}).text.strip() for book in books]\n",
    "\n",
    "books_dict = {'Title': titles, 'Price': prices, 'Stock': stock}\n",
    "\n",
    "books_df = pd.DataFrame(books_dict)\n",
    "books_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
